Assignment: GS 540 HW7
Name: Vincent Chau
Email: vkchau@uw.edu
Language: C++17
Runtime: 2m25.799s

Background frequencies:
0=0.8773
1=0.1126
2=0.0090
>=3=0.0010

Target frequencies:
0=0.7555
1=0.1882
2=0.0301
>=3=0.0263

Scoring scheme:
0=-0.2157
1=0.7402
2=1.7443
>=3=4.6481

Real data:
5 71549
6 39390
7 22553
8 13954
9 9257
10 6577
11 4951
12 3967
13 3335
14 2861
15 2512
16 2237
17 2068
18 1898
19 1754
20 1625
21 1536
22 1459
23 1374
24 1311
25 1239
26 1177
27 1127
28 1082
29 1026
30 984


Simulated data:
5 80008
6 39373
7 18792
8 9085
9 4541
10 2246
11 1108
12 550
13 275
14 142
15 57
16 34
17 13
18 8
19 4
20 0
21 0
22 0
23 0
24 0
25 0
26 0
27 0
28 0
29 0
30 0


Ratios of simulated data:
N_seg(5)/N_seg(6) 2.03
N_seg(6)/N_seg(7) 2.10
N_seg(7)/N_seg(8) 2.07
N_seg(8)/N_seg(9) 2.00
N_seg(9)/N_seg(10) 2.02
N_seg(10)/N_seg(11) 2.03
N_seg(11)/N_seg(12) 2.01
N_seg(12)/N_seg(13) 2.00
N_seg(13)/N_seg(14) 1.94
N_seg(14)/N_seg(15) 2.49
N_seg(15)/N_seg(16) 1.68
N_seg(16)/N_seg(17) 2.62
N_seg(17)/N_seg(18) 1.62
N_seg(18)/N_seg(19) 2.00
N_seg(19)/N_seg(20) -1.00
N_seg(20)/N_seg(21) -1.00
N_seg(21)/N_seg(22) -1.00
N_seg(22)/N_seg(23) -1.00
N_seg(23)/N_seg(24) -1.00
N_seg(24)/N_seg(25) -1.00
N_seg(25)/N_seg(26) -1.00
N_seg(26)/N_seg(27) -1.00
N_seg(27)/N_seg(28) -1.00
N_seg(28)/N_seg(29) -1.00
N_seg(29)/N_seg(30) -1.00


As discussed in lecture, Karlin-Altschul theory predicts that, for LLR scores using logarithmic base b, the number of D-segments with scores >= s should be proportional to b^-s (b to the power -s; this is the reciprocal of the corresponding LR). Since your scores used logarithmic base 2, if N_seg(s1) is the number of D-segments found with score value >= s1, and N_seg(s2) is the number of D-segments found with score value >= s2, then the ratio N_seg(s1)/N_seg(s2) should be approximately equal to 2^(s2 - s1). Consider the following questions:
Does this relationship appear to be true for the simulated data? Yes, 2^(s2 - s1) = 2. The valid ratios of of the simulated data all tend towards 2.
Is it true for the real data? No, the real data does not exhibit the same proportion.
Would you expect it to be true for the real data? No.
What score threshold is a reasonable one to use for the real data, to ensure a very low false positive rate? S = -D = 20 because this is the start of when >= 0 segments reach the score threshold in the simulated data (meaning that segments that pass this threshold are very likely to be statistically significant).

Program:

#include <bits/stdc++.h>

using namespace std;

int main() {
    ios::sync_with_stdio(false);
    cin.tie(NULL);

    freopen("output.txt", "a", stdout);
    string input = "input/hw_06.txt";

    freopen(input.c_str(), "r", stdin);

    int n_count = 8422401;

    string line;
    map<int, int> bgHist;
    map<int, int> targHist;
    int record = 0;
    while (getline(cin, line, '\n')) {
        if (line.empty()) {
            record = 0;
        } else if (line ==
                   "Read start histogram for non-elevated copy-number "
                   "segments:") {
            record = 1;
        } else if (record == 1) {
            bgHist[stoi(line.substr(line.find_first_of("0123456789"), 1))] =
                stoi(line.substr(line.rfind('=') + 1));
        } else if (line ==
                   "Read start histogram for elevated copy-number segments:") {
            record = 2;
        } else if (record == 2) {
            targHist[stoi(line.substr(line.find_first_of("0123456789"), 1))] =
                stoi(line.substr(line.rfind('=') + 1));
        }
    }
    bgHist[0] -= n_count;
    assert(bgHist.size() == targHist.size());

    int bgSum = accumulate(
        bgHist.begin(), bgHist.end(), 0,
        [](const int prev, const auto& v) { return prev + v.second; });
    int targSum = accumulate(
        targHist.begin(), targHist.end(), 0,
        [](const int prev, const auto& v) { return prev + v.second; });
    int bgTargSum = bgSum + targSum;

    map<int, double> bgFreq;
    map<int, double> targFreq;
    for (auto& [k, v] : bgHist) {
        bgFreq[k] = static_cast<double>(v + targHist[k]) / (bgTargSum);
    }
    for (auto& [k, v] : targHist) {
        targFreq[k] = static_cast<double>(v) / targSum;
    }

    map<int, double> score;
    for (auto& [k, v] : targFreq) {
        score[k] = log2(v / bgFreq[k]);
    }

    string scoreName = "scoring.scheme.txt";
    ofstream scoreFile("input/" + scoreName);
    for (auto& [k, v] : score) {
        if (k == next(score.rend())->first) {
            scoreFile << ">=";
        }
        scoreFile << k << ' ' << v << '\n';
    }
    scoreFile << "D -5" << '\n';
    scoreFile << "S 5" << '\n';
    scoreFile.close();

    string seqName = "chm13.chr16.txt";
    ofstream seqFile("input/" + seqName);
    string gen;
    random_device rd;
    mt19937 ranGen(1337);
    vector<double> bgFreqVal;
    for (auto& [k, v] : bgFreq) {
        bgFreqVal.push_back(v);
    }
    discrete_distribution<> dd(bgFreqVal.begin(), bgFreqVal.end());
    for (int i = 0; i < bgTargSum; i++) {
        seqFile << "16" << '\t' << i + 1 << '\t' << dd(ranGen) << '\n';
    }
    seqFile.close();

    cout << fixed << setprecision(4);
    cout << "Background frequencies:" << '\n';
    for (auto& [k, v] : bgFreq) {
        if (k == next(bgFreq.rend())->first) {
            cout << ">=";
        }
        cout << k << "={" << v << '}' << '\n';
    }
    cout << '\n';
    cout << "Target frequencies:" << '\n';
    for (auto& [k, v] : targFreq) {
        if (k == next(targFreq.rend())->first) {
            cout << ">=";
        }
        cout << k << "={" << v << '}' << '\n';
    }
    cout << '\n';
    cout << "Scoring scheme:" << '\n';
    for (auto& [k, v] : score) {
        if (k == next(score.rend())->first) {
            cout << ">=";
        }
        cout << k << "={" << v << '}' << '\n';
    }
    cout << '\n';

    return 0;
}